{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl (42kB)\r\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \r\n",
      "\u001b[?25hCollecting pretrainedmodels==0.7.4\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\r\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \r\n",
      "\u001b[?25hCollecting efficientnet-pytorch>=0.5.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/b7/c7cc9d8a95b7cfe4392c28fd5ff1854061aa75e81acf8d7539adb7612f20/efficientnet_pytorch-0.6.1.tar.gz\r\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from segmentation-models-pytorch) (0.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.4.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.41.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.14.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (5.4.1)\r\n",
      "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=9a8453086c2943f7bd43337c95c70eda482eb61739e3277ff61763987a4143c3\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.1-cp36-none-any.whl size=12407 sha256=89ac9fef9733727cdd1f2b7d4e1ce1b06269fc75a48a2125157d18f69325f3cd\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/b0/79/1663bc9714b5d88deba05e44f593ce50111bd69305a45df87a\r\n",
      "Successfully built pretrainedmodels efficientnet-pytorch\r\n",
      "Installing collected packages: pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.6.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import collections\n",
    "import albumentations as albu\n",
    "import torchvision\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "NUM_CLASSES = 46\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "N_WORKERS = 2\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "root_path_train = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train'\n",
    "df_path_train = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated: [start0] [length0] [start1] [length1]... in 1d array\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array according to the shape, 1 - mask, 0 - background\n",
    "    '''\n",
    "    shape = (shape[1], shape[0])\n",
    "    s = mask_rle.split()\n",
    "    # gets starts & lengths 1d arrays\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
    "    starts -= 1\n",
    "    # gets ends 1d array\n",
    "    ends = starts + lengths\n",
    "    # creates blank mask image 1d array\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # sets mark pixles\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    # reshape as a 2d mask image\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def create_one_represent_class(df_param):\n",
    "    v_c_df = df_param['CategoryId'].value_counts().reset_index()\n",
    "    one_represent = v_c_df.loc[v_c_df['CategoryId'] == 1, 'index'].tolist()\n",
    "    df_param.loc[df_param['CategoryId'].isin(one_represent), 'CategoryId'] = 'one_represent'\n",
    "    return df_param\n",
    "\n",
    "def custom_train_test_split(df_param):\n",
    "    \n",
    "    df_param['CategoryId'] = df_param.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n",
    "    \n",
    "    img_categ = train_df.groupby('ImageId')['CategoryId'].apply(list).reset_index()\n",
    "    img_categ['CategoryId'] = img_categ['CategoryId'].apply(lambda x: ' '.join(sorted(x)))\n",
    "    \n",
    "    img_categ = create_one_represent_class(img_categ)\n",
    "    \n",
    "    img_train, img_val  = train_test_split(img_categ, test_size=0.2, random_state=42, stratify=img_categ['CategoryId'])\n",
    "    \n",
    "    df_param = df_param.drop(columns='CategoryId')\n",
    "    \n",
    "    df_train = df_param[df_param['ImageId'].isin(img_train['ImageId'])].reset_index(drop=True)\n",
    "    df_val = df_param[df_param['ImageId'].isin(img_val['ImageId'])].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_class_id_df(inital_df):\n",
    "    temp_df = inital_df.groupby(['ImageId','ClassId'])['EncodedPixels'].agg(lambda x: ' '.join(list(x))).reset_index()\n",
    "    size_df = inital_df.groupby(['ImageId','ClassId'])['Height', 'Width'].mean().reset_index()\n",
    "    temp_df = temp_df.merge(size_df, on=['ImageId','ClassId'], how='left')\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>6068157 7 6073371 20 6078584 34 6083797 48 608...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>6323163 11 6328356 32 6333549 53 6338742 75 63...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>8521389 10 8526585 30 8531789 42 8537002 46 85...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>12903854 2 12909064 7 12914275 10 12919485 15 ...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>10837337 5 10842542 14 10847746 24 10852951 33...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ImageId  \\\n",
       "0  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "1  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "2  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "3  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "4  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "\n",
       "                                       EncodedPixels  Height  Width ClassId  \n",
       "0  6068157 7 6073371 20 6078584 34 6083797 48 608...    5214   3676       6  \n",
       "1  6323163 11 6328356 32 6333549 53 6338742 75 63...    5214   3676       0  \n",
       "2  8521389 10 8526585 30 8531789 42 8537002 46 85...    5214   3676      28  \n",
       "3  12903854 2 12909064 7 12914275 10 12919485 15 ...    5214   3676      31  \n",
       "4  10837337 5 10842542 14 10847746 24 10852951 33...    5214   3676      32  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(df_path_train)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = custom_train_test_split(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = get_unique_class_id_df(train_df), get_unique_class_id_df(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, df, height, width, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n",
    "        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x)).reset_index()\n",
    "        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n",
    "        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n",
    "        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):\n",
    "            image_id = row['ImageId']\n",
    "            image_path = os.path.join(self.image_dir, image_id)\n",
    "            self.image_info[index][\"image_id\"] = image_id\n",
    "            self.image_info[index][\"image_path\"] = image_path\n",
    "            self.image_info[index][\"width\"] = self.width\n",
    "            self.image_info[index][\"height\"] = self.height\n",
    "            self.image_info[index][\"labels\"] = row[\"CategoryId\"]\n",
    "            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n",
    "            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n",
    "            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]\n",
    "            \n",
    "        self.img2tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)\n",
    "        labels = []\n",
    "        for m, (annotation, label) in enumerate(zip(info['annotations'], info['labels'])):\n",
    "            sub_mask = rle_decode(annotation, (info['orig_height'], info['orig_width']))\n",
    "            sub_mask = Image.fromarray(sub_mask)\n",
    "            sub_mask = sub_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n",
    "            mask[m, :, :] = sub_mask\n",
    "            labels.append(int(label) + 1)\n",
    "\n",
    "        num_objs = len(labels)\n",
    "        boxes = []\n",
    "        new_labels = []\n",
    "        new_masks = []\n",
    "\n",
    "        for i in range(num_objs):\n",
    "            try:\n",
    "                pos = np.where(mask[i, :, :])\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                if abs(xmax - xmin) >= 20 and abs(ymax - ymin) >= 20:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    new_labels.append(labels[i])\n",
    "                    new_masks.append(mask[i, :, :])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if len(new_labels) == 0:\n",
    "            boxes.append([0, 0, 20, 20])\n",
    "            new_labels.append(0)\n",
    "            new_masks.append(mask[0, :, :])\n",
    "\n",
    "        nmx = np.zeros((len(new_masks), self.width, self.height), dtype=np.uint8)\n",
    "        for i, n in enumerate(new_masks):\n",
    "            nmx[i, :, :] = n\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(new_labels, dtype=torch.int64)\n",
    "        target[\"masks\"] = torch.as_tensor(nmx, dtype=torch.uint8)\n",
    "        \n",
    "        img = self.img2tensor(img)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img, label in batch:\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36156/36156 [00:08<00:00, 4305.96it/s]\n"
     ]
    }
   ],
   "source": [
    "datset = FashionDataset(image_dir=root_path_train, \n",
    "                        df=train_df, \n",
    "                        height=IMG_HEIGHT, \n",
    "                        width=IMG_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datset, batch_size=4, shuffle=True, num_workers=2,\n",
    "    collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEpoch(smp.utils.train.Epoch):\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def run(self, dataloader):\n",
    "\n",
    "        self.on_epoch_start()\n",
    "\n",
    "        logs = {}\n",
    "        loss_meter = smp.utils.meter.AverageValueMeter()\n",
    "        \n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for x, y in iterator:\n",
    "                x = list(map(lambda x_el: x_el.to(self.device), x))\n",
    "                y = list(map(lambda y_el: {k:v.to(self.device) for k,v in y_el.items()}, y))\n",
    "                loss = self.batch_update(x, y)\n",
    "\n",
    "                # update loss logs\n",
    "                loss_value = loss.cpu().detach().numpy()\n",
    "                loss_meter.add(loss_value)\n",
    "                loss_logs = {'loss': loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "\n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "\n",
    "        return logs\n",
    "    \n",
    "class TrainEpoch(MyEpoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.model(x, y)\n",
    "        loss = sum(l for l in loss.values())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27536d0472514ccba2f703c1dde14cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178090079.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_classes = NUM_CLASSES + 1\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "model_ft = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model_ft.roi_heads.box_predictor.cls_score.in_features\n",
    "model_ft.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model_ft.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model_ft.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = TrainEpoch(\n",
    "    model_ft, \n",
    "    loss=None, \n",
    "    metrics=None, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 9039/9039 [4:25:39<00:00,  1.76s/it, loss - 0.9849]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.9849315819776964}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch.run(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "206de72379e54a80aea60106ae5df594": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25a7336fd8724d89abdcb021f08c73a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "27536d0472514ccba2f703c1dde14cc4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_707ef56f6a884246804eefbb6d742f85",
        "IPY_MODEL_558d31bf15134d7fa2841e00a0018561"
       ],
       "layout": "IPY_MODEL_206de72379e54a80aea60106ae5df594"
      }
     },
     "558d31bf15134d7fa2841e00a0018561": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_868233b33b5446feb60800180431e756",
       "placeholder": "​",
       "style": "IPY_MODEL_eff15f174c6948ab9be50a63bf87c885",
       "value": " 170M/170M [00:02&lt;00:00, 82.7MB/s]"
      }
     },
     "707ef56f6a884246804eefbb6d742f85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_25a7336fd8724d89abdcb021f08c73a7",
       "max": 178090079.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ef6ef6b405344c0a8031f3244608419a",
       "value": 178090079.0
      }
     },
     "868233b33b5446feb60800180431e756": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef6ef6b405344c0a8031f3244608419a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "eff15f174c6948ab9be50a63bf87c885": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
