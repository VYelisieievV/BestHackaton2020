{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl (42kB)\r\n",
      "\u001b[K     |████████████████████████████████| 51kB 3.2MB/s \r\n",
      "\u001b[?25hCollecting pretrainedmodels==0.7.4\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\r\n",
      "\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \r\n",
      "\u001b[?25hCollecting efficientnet-pytorch>=0.5.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/b7/c7cc9d8a95b7cfe4392c28fd5ff1854061aa75e81acf8d7539adb7612f20/efficientnet_pytorch-0.6.1.tar.gz\r\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from segmentation-models-pytorch) (0.4.1a0+d94043a)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.3.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.40.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.13.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (5.4.1)\r\n",
      "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=422ade2f38bac938f81d43f2d4aac10b7ea00e29d0be3a494489fb84778056f5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.1-cp36-none-any.whl size=12407 sha256=aaa84beb3bc5b4ad28128ccafa98b24a1f124ed8b2e82f5e09b066d4c50d31c1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0f/b0/79/1663bc9714b5d88deba05e44f593ce50111bd69305a45df87a\r\n",
      "Successfully built pretrainedmodels efficientnet-pytorch\r\n",
      "Installing collected packages: pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.6.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import collections\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as albu\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "NUM_CLASSES = 46\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "N_WORKERS = 2\n",
    "\n",
    "root_path_train = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train'\n",
    "df_path_train = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_represent_class(df_param):\n",
    "    v_c_df = df_param['CategoryId'].value_counts().reset_index()\n",
    "    one_represent = v_c_df.loc[v_c_df['CategoryId'] == 1, 'index'].tolist()\n",
    "    df_param.loc[df_param['CategoryId'].isin(one_represent), 'CategoryId'] = 'one_represent'\n",
    "    return df_param\n",
    "\n",
    "def custom_train_test_split(df_param):\n",
    "    \n",
    "    df_param['CategoryId'] = df_param.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n",
    "    \n",
    "    img_categ = train_df.groupby('ImageId')['CategoryId'].apply(list).reset_index()\n",
    "    img_categ['CategoryId'] = img_categ['CategoryId'].apply(lambda x: ' '.join(sorted(x)))\n",
    "    \n",
    "    img_categ = create_one_represent_class(img_categ)\n",
    "    \n",
    "    img_train, img_val  = train_test_split(img_categ, test_size=0.2, random_state=42, stratify=img_categ['CategoryId'])\n",
    "    \n",
    "    df_param = df_param.drop(columns='CategoryId')\n",
    "    \n",
    "    df_train = df_param[df_param['ImageId'].isin(img_train['ImageId'])].reset_index(drop=True)\n",
    "    df_val = df_param[df_param['ImageId'].isin(img_val['ImageId'])].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>6068157 7 6073371 20 6078584 34 6083797 48 608...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>6323163 11 6328356 32 6333549 53 6338742 75 63...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>8521389 10 8526585 30 8531789 42 8537002 46 85...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>12903854 2 12909064 7 12914275 10 12919485 15 ...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n",
       "      <td>10837337 5 10842542 14 10847746 24 10852951 33...</td>\n",
       "      <td>5214</td>\n",
       "      <td>3676</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ImageId  \\\n",
       "0  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "1  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "2  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "3  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "4  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n",
       "\n",
       "                                       EncodedPixels  Height  Width ClassId  \n",
       "0  6068157 7 6073371 20 6078584 34 6083797 48 608...    5214   3676       6  \n",
       "1  6323163 11 6328356 32 6333549 53 6338742 75 63...    5214   3676       0  \n",
       "2  8521389 10 8526585 30 8531789 42 8537002 46 85...    5214   3676      28  \n",
       "3  12903854 2 12909064 7 12914275 10 12919485 15 ...    5214   3676      31  \n",
       "4  10837337 5 10842542 14 10847746 24 10852951 33...    5214   3676      32  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(df_path_train)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = custom_train_test_split(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated: [start0] [length0] [start1] [length1]... in 1d array\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array according to the shape, 1 - mask, 0 - background\n",
    "    '''\n",
    "    shape = (shape[1], shape[0])\n",
    "    s = mask_rle.split()\n",
    "    # gets starts & lengths 1d arrays\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
    "    starts -= 1\n",
    "    # gets ends 1d array\n",
    "    ends = starts + lengths\n",
    "    # creates blank mask image 1d array\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    # sets mark pixles\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    # reshape as a 2d mask image\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "    \n",
    "class UnetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, df, height, width, augmentation=None, preprocessing=None):\n",
    "        \n",
    "        self.preprocessing = preprocessing\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        \n",
    "        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n",
    "        self.num_classes = self.df['CategoryId'].nunique()\n",
    "        \n",
    "        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x)).reset_index()\n",
    "        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n",
    "        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n",
    "        \n",
    "        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):\n",
    "            image_id = row['ImageId']\n",
    "            image_path = os.path.join(self.image_dir, image_id)\n",
    "            self.image_info[index][\"image_id\"] = image_id\n",
    "            self.image_info[index][\"image_path\"] = image_path\n",
    "            self.image_info[index][\"width\"] = self.width\n",
    "            self.image_info[index][\"height\"] = self.height\n",
    "            self.image_info[index][\"labels\"] = row[\"CategoryId\"]\n",
    "            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n",
    "            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n",
    "            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.width, self.height))\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "        \n",
    "        mask = np.zeros((self.width, self.height, self.num_classes))\n",
    "        labels = np.zeros(self.num_classes)\n",
    "        \n",
    "        for annotation, label in zip(info['annotations'], info['labels']):\n",
    "            cur_mask = rle_decode(annotation, (info['orig_height'], info['orig_width']))\n",
    "            mask[:, :, int(label)] += cv2.resize(cur_mask, (self.width, self.height))\n",
    "            labels[int(label)] = 1\n",
    "            \n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation is not None:\n",
    "            sample = self.augmentation(image=img, mask=mask)\n",
    "            img, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing is not None:\n",
    "            sample = self.preprocessing(image=img, mask=mask)\n",
    "            img, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return img, mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n",
    "\n",
    "    \n",
    "def collate_function(batch):\n",
    "    image_array = torch.zeros((len(batch), batch[0][0].shape[0], batch[0][0].shape[1], batch[0][0].shape[2]))\n",
    "    mask_array = torch.zeros((len(batch), batch[0][1].shape[0], batch[0][1].shape[1], batch[0][1].shape[2]))\n",
    "    label_array = torch.zeros((len(batch), batch[0][2].shape[0]))\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        image_array[i,:,:,:] = torch.Tensor(batch[i][0])\n",
    "        mask_array[i,:,:,:] = torch.Tensor(batch[i][1])\n",
    "        label_array[i,:] = torch.Tensor(batch[i][2])\n",
    "        \n",
    "    return image_array, (mask_array, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstHeadDiceSecondHeadBCE(smp.utils.base.Loss):\n",
    "    def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dice_loss = smp.utils.losses.DiceLoss(eps=1., beta=1., activation=None, ignore_channels=None, **kwargs)\n",
    "        self.bce = smp.utils.losses.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, y_pr, y_gt):\n",
    "        return self.dice_loss(y_pr[0], y_gt[0]) + self.bce(y_pr[1], y_gt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEpoch(smp.utils.train.Epoch):\n",
    "    def run(self, dataloader):\n",
    "\n",
    "        self.on_epoch_start()\n",
    "\n",
    "        logs = {}\n",
    "        loss_meter = smp.utils.meter.AverageValueMeter()\n",
    "        metrics_meters = {metric.__name__: smp.utils.meter.AverageValueMeter() for metric in self.metrics}\n",
    "\n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for x, y in iterator:\n",
    "                x, y = x.to(self.device), (y[0].to(self.device), y[1].to(self.device))\n",
    "                loss, y_pred = self.batch_update(x, y)\n",
    "\n",
    "                # update loss logs\n",
    "                loss_value = loss.cpu().detach().numpy()\n",
    "                loss_meter.add(loss_value)\n",
    "                loss_logs = {self.loss.__name__: loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "\n",
    "                # update metrics logs\n",
    "                for metric_fn in self.metrics:\n",
    "                    metric_value = metric_fn(y_pred[0], y[0]).cpu().detach().numpy()\n",
    "                    metrics_meters[metric_fn.__name__].add(metric_value)\n",
    "                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}\n",
    "                logs.update(metrics_logs)\n",
    "\n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "\n",
    "        return logs\n",
    "    \n",
    "class TrainEpoch(MyEpoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.model.forward(x)\n",
    "        loss = self.loss(prediction, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss, prediction\n",
    "    \n",
    "class ValidEpoch(MyEpoch):\n",
    "\n",
    "    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='valid',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model.forward(x)\n",
    "            loss = self.loss(prediction, y)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'mobilenet_v2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "aux_params=dict(\n",
    "    pooling='avg',             \n",
    "    dropout=0.2,               \n",
    "    activation=None,      \n",
    "    classes=NUM_CLASSES,                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/checkpoints/mobilenet_v2-b0353104.pth\n",
      "100%|██████████| 13.6M/13.6M [00:00<00:00, 56.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=NUM_CLASSES, \n",
    "    activation=ACTIVATION,\n",
    "    aux_params=aux_params\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n",
      "100%|██████████| 36156/36156 [00:08<00:00, 4140.06it/s]\n",
      "100%|██████████| 9039/9039 [00:02<00:00, 3930.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = UnetDataset(\n",
    "    root_path_train,\n",
    "    train_df,\n",
    "    IMG_HEIGHT,\n",
    "    IMG_WIDTH, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    augmentation=get_training_augmentation()\n",
    ")\n",
    "\n",
    "valid_dataset = UnetDataset(\n",
    "    root_path_train,\n",
    "    val_df,\n",
    "    IMG_HEIGHT,\n",
    "    IMG_WIDTH, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=N_WORKERS, collate_fn=collate_function)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=N_WORKERS, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = FirstHeadDiceSecondHeadBCE()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 1130/1130 [2:31:26<00:00,  8.04s/it, first_head_dice_second_head_bce - 0.8258, iou_score - 0.2506]\n",
      "valid: 100%|██████████| 283/283 [36:03<00:00,  7.64s/it, first_head_dice_second_head_bce - 0.7188, iou_score - 0.3195]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 1130/1130 [2:35:26<00:00,  8.25s/it, first_head_dice_second_head_bce - 0.7142, iou_score - 0.3244]\n",
      "valid: 100%|██████████| 283/283 [35:30<00:00,  7.53s/it, first_head_dice_second_head_bce - 0.6948, iou_score - 0.3351]\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'best_model.pth')\n",
    "max_score = 0\n",
    "\n",
    "for i in range(0, 2):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
