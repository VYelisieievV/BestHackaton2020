{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pip installs"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install segmentation-models-pytorch","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting segmentation-models-pytorch\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/88/763a25dfe076a9f30f33466b1bd0f2d31b915b88d4cb4481fe4043cf26b4/segmentation_models_pytorch-0.1.0-py3-none-any.whl (42kB)\n\u001b[K     |████████████████████████████████| 51kB 1.9MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from segmentation-models-pytorch) (0.4.1a0+d94043a)\nCollecting efficientnet-pytorch>=0.5.1\n  Downloading https://files.pythonhosted.org/packages/e5/b7/c7cc9d8a95b7cfe4392c28fd5ff1854061aa75e81acf8d7539adb7612f20/efficientnet_pytorch-0.6.1.tar.gz\nCollecting pretrainedmodels==0.7.4\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n\u001b[K     |████████████████████████████████| 61kB 2.9MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.18.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.13.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.3.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (5.4.1)\nRequirement already satisfied: munch in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.40.2)\nBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.1-cp36-none-any.whl size=12407 sha256=b82648e69945591051e30d457cf6d3ec92306c8b22ffad9df8324f141f309b1b\n  Stored in directory: /root/.cache/pip/wheels/0f/b0/79/1663bc9714b5d88deba05e44f593ce50111bd69305a45df87a\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60963 sha256=ca440e63b4d41ac1f5fa447be497650545b4e4d2a3cb8e65bbb05c4f8eccfd79\n  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\nSuccessfully built efficientnet-pytorch pretrainedmodels\nInstalling collected packages: efficientnet-pytorch, pretrainedmodels, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.1 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nimport collections\nimport segmentation_models_pytorch as smp\nimport albumentations as albu\n\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom os import path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_PATH_TRAIN = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train'\nDF_PATH_TRAIN = '/kaggle/input/imaterialist-fashion-2019-FGVC6/train.csv'\nPATH_TO_MODEL_WEIGHTS = '/kaggle/input/za-cho-takoe-testovoe/best_model.pth'\n\nIMAGE_PREDICTION_SIZE = (256, 256)\nN_CLASSES = 46","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils fucntions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated: [start0] [length0] [start1] [length1]... in 1d array\n    shape: (height,width) of array to return\n    Returns numpy array according to the shape, 1 - mask, 0 - background\n    '''\n    shape = (shape[1], shape[0])\n    s = mask_rle.split()\n    # gets starts & lengths 1d arrays\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n    starts -= 1\n    # gets ends 1d array\n    ends = starts + lengths\n    # creates blank mask image 1d array\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    # sets mark pixles\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    # reshape as a 2d mask image\n    return img.reshape(shape).T  # Needed to align to RLE direction","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_one_represent_class(df_param):\n    v_c_df = df_param['CategoryId'].value_counts().reset_index()\n    one_represent = v_c_df.loc[v_c_df['CategoryId'] == 1, 'index'].tolist()\n    df_param.loc[df_param['CategoryId'].isin(one_represent), 'CategoryId'] = 'one_represent'\n    return df_param\n\ndef custom_train_test_split(df_param):\n    \n    df_param['CategoryId'] = df_param.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n    \n    img_categ = train_df.groupby('ImageId')['CategoryId'].apply(list).reset_index()\n    img_categ['CategoryId'] = img_categ['CategoryId'].apply(lambda x: ' '.join(sorted(x)))\n    \n    img_categ = create_one_represent_class(img_categ)\n    \n    img_train, img_val  = train_test_split(img_categ, test_size=0.2, random_state=42, stratify=img_categ['CategoryId'])\n    \n    df_param = df_param.drop(columns='CategoryId')\n    \n    df_train = df_param[df_param['ImageId'].isin(img_train['ImageId'])].reset_index(drop=True)\n    df_val = df_param[df_param['ImageId'].isin(img_val['ImageId'])].reset_index(drop=True)\n    \n    return df_train, df_val","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(DF_PATH_TRAIN)\ntrain_df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"                                ImageId  \\\n0  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n1  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n2  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n3  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n4  00000663ed1ff0c4e0132b9b9ac53f6e.jpg   \n\n                                       EncodedPixels  Height  Width ClassId  \n0  6068157 7 6073371 20 6078584 34 6083797 48 608...    5214   3676       6  \n1  6323163 11 6328356 32 6333549 53 6338742 75 63...    5214   3676       0  \n2  8521389 10 8526585 30 8531789 42 8537002 46 85...    5214   3676      28  \n3  12903854 2 12909064 7 12914275 10 12919485 15 ...    5214   3676      31  \n4  10837337 5 10842542 14 10847746 24 10852951 33...    5214   3676      32  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>EncodedPixels</th>\n      <th>Height</th>\n      <th>Width</th>\n      <th>ClassId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>6068157 7 6073371 20 6078584 34 6083797 48 608...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>6323163 11 6328356 32 6333549 53 6338742 75 63...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>8521389 10 8526585 30 8531789 42 8537002 46 85...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>12903854 2 12909064 7 12914275 10 12919485 15 ...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00000663ed1ff0c4e0132b9b9ac53f6e.jpg</td>\n      <td>10837337 5 10842542 14 10847746 24 10852951 33...</td>\n      <td>5214</td>\n      <td>3676</td>\n      <td>32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = custom_train_test_split(train_df)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class UnetDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, df, height, width, augmentation=None, preprocessing=None):\n        \n        self.preprocessing = preprocessing\n        self.augmentation = augmentation\n        \n        self.image_dir = image_dir\n        self.df = df\n        \n        self.height = height\n        self.width = width\n        \n        self.image_info = collections.defaultdict(dict)\n        \n        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n        self.num_classes = self.df['CategoryId'].nunique()\n        \n        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x)).reset_index()\n        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n        \n        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):\n            image_id = row['ImageId']\n            image_path = os.path.join(self.image_dir, image_id)\n            self.image_info[index][\"image_id\"] = image_id\n            self.image_info[index][\"image_path\"] = image_path\n            self.image_info[index][\"width\"] = self.width\n            self.image_info[index][\"height\"] = self.height\n            self.image_info[index][\"labels\"] = row[\"CategoryId\"]\n            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]\n\n    def __getitem__(self, idx):\n        \n        img_path = self.image_info[idx][\"image_path\"]\n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (self.width, self.height))\n\n        info = self.image_info[idx]\n        \n        mask = np.zeros((self.width, self.height, self.num_classes))\n        \n        for annotation, label in zip(info['annotations'], info['labels']):\n            cur_mask = rle_decode(annotation, (info['orig_height'], info['orig_width']))\n            mask[:, :, int(label)] += cv2.resize(cur_mask, (self.width, self.height))\n            \n        mask = (mask > 0.5).astype(np.float32)\n        \n        # apply augmentations\n        if self.augmentation is not None:\n            sample = self.augmentation(image=img, mask=mask)\n            img, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing is not None:\n            sample = self.preprocessing(image=img, mask=mask)\n            img, mask = sample['image'], sample['mask']\n            \n        return img, mask\n\n    def __len__(self):\n        return len(self.image_info)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        albu.HorizontalFlip(p=0.5),\n        albu.VerticalFlip(p=0.5),\n    ]\n    return albu.Compose(train_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)\n","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ENCODER = 'mobilenet_v2'\nENCODER_WEIGHTS = 'imagenet'\nDEVICE = 'cuda'\n\nACTIVATION = 'sigmoid'","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=N_CLASSES, \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","execution_count":15,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/checkpoints/mobilenet_v2-b0353104.pth\n100%|██████████| 13.6M/13.6M [00:00<00:00, 54.4MB/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = UnetDataset(\n    ROOT_PATH_TRAIN,\n    train_df,\n    IMAGE_PREDICTION_SIZE[0],\n    IMAGE_PREDICTION_SIZE[1], \n    preprocessing=get_preprocessing(preprocessing_fn),\n    augmentation=get_training_augmentation()\n)\n\nvalid_dataset = UnetDataset(\n    ROOT_PATH_TRAIN,\n    val_df,\n    IMAGE_PREDICTION_SIZE[0],\n    IMAGE_PREDICTION_SIZE[1], \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)","execution_count":16,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'root_path_train' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b84d0adc95cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset = UnetDataset(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mroot_path_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mIMAGE_PREDICTION_SIZE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mIMAGE_PREDICTION_SIZE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'root_path_train' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"# Loss Metric Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = smp.utils.losses.DiceLoss()\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Epoch class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create epoch runners \n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'best_model.pth')\nmax_score = 0\n\nfor i in range(0, 2):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    \n    # do something (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(model.state_dict(), 'best_model.pth')\n        print('Model saved!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}